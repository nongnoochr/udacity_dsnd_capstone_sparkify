{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import avg, col, desc, min, max, udf\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.feature import StringIndexerModel, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session locally\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local') \\\n",
    "    .appName('Churn Prediction') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "Clean your dataset, checking for invalid or missing data. For example, records without userids or sessionids. In this workspace, the filename is `mini_sparkify_event_data.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data = 'mini_sparkify_event_data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: bigint, lastName: string, length: double, level: string, location: string, method: string, page: string, registration: bigint, sessionId: bigint, song: string, status: bigint, ts: bigint, userAgent: string, userId: string]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.json(event_data)\n",
    "df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the data \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286500"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find a total number of rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------+---------+------+------------------+--------+-----------------+------+-----------------+------+-------+--------------------+-----------------+--------------------+------------------+--------------------+--------------------+-----------------+\n",
      "|summary|            artist|      auth|firstName|gender|     itemInSession|lastName|           length| level|         location|method|   page|        registration|        sessionId|                song|            status|                  ts|           userAgent|           userId|\n",
      "+-------+------------------+----------+---------+------+------------------+--------+-----------------+------+-----------------+------+-------+--------------------+-----------------+--------------------+------------------+--------------------+--------------------+-----------------+\n",
      "|  count|            228108|    286500|   278154|278154|            286500|  278154|           228108|286500|           278154|286500| 286500|              278154|           286500|              228108|            286500|              286500|              278154|           286500|\n",
      "|   mean| 551.0852017937219|      null|     null|  null|114.41421291448516|    null|249.1171819778458|  null|             null|  null|   null|1.535358834084427...|1041.526554973822|            Infinity|210.05459685863875|1.540956889810483...|                null|59682.02278593872|\n",
      "| stddev|1217.7693079161374|      null|     null|  null|129.76726201140994|    null|99.23517921058361|  null|             null|  null|   null| 3.291321616327586E9|726.7762634630741|                 NaN| 31.50507848842214|1.5075439608226302E9|                null|109091.9499991047|\n",
      "|    min|               !!!| Cancelled| Adelaida|     F|                 0|   Adams|          0.78322|  free|       Albany, OR|   GET|  About|       1521380675000|                1|\u001c",
      "ÃÂg ÃÂtti Gr...|               200|       1538352117000|\"Mozilla/5.0 (Mac...|                 |\n",
      "|    max| ÃÂlafur Arnalds|Logged Out|   Zyonna|     M|              1321|  Wright|       3024.66567|  paid|Winston-Salem, NC|   PUT|Upgrade|       1543247354000|             2474|ÃÂau hafa slopp...|               404|       1543799476000|Mozilla/5.0 (comp...|               99|\n",
      "+-------+------------------+----------+---------+------+------------------+--------+-----------------+------+-----------------+------+-------+--------------------+-----------------+--------------------+------------------+--------------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the statistics\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look atcounts of each column\n",
    "df.describe().show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect unique available userIds\n",
    "df.select('userId').dropDuplicates().sort('userId').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('userId').groupby(df.userId).count().sort('userId').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values in the userId columns\n",
    "df_valid = df.filter(df.userId != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the counts of each column again\n",
    "df_valid.describe().show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a udf function to identify churn by flagging an event \n",
    "# with the \"Cancellation Confirmation\" page\n",
    "\n",
    "is_churned = udf(lambda x: 1 if x == \"Cancellation Confirmation\" else 0, IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'isChurned' by passing a value in the 'page' column to the udf function\n",
    "# created above (is_churned)\n",
    "df_valid = df_valid.withColumn('isChurned', is_churned(\"page\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, inspect the data\n",
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, Identify events of users who are churned by \n",
    "# 1. Create a window object partitioned by a userId, ordered descendingly by a timestamp 'ts'\n",
    "# 2. Then, utilize the pyspark.sql.functions.sum function (Fsum) to perform an accumulate sum\n",
    "# within a window and assign its value to a new column 'willBeChurned'\n",
    "# 3. With the steps above, events with users who are churned \n",
    "\n",
    "windowval = Window.partitionBy(\"userId\") \\\n",
    "                .orderBy(desc(\"ts\")) \\\n",
    "                .rangeBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "df_valid = df_valid.withColumn(\"willBeChurned\", Fsum(\"isChurned\").over(windowval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a datetime to help with the analysis\n",
    "get_datetime = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "df_valid = df_valid.withColumn('datetime', get_datetime(df_valid.ts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the value\n",
    "df_valid.filter('willBeChurned == 1') \\\n",
    "    .select('userId', 'datetime', 'page', 'level', 'willBeChurned') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare page counts of events with and without churns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comparison plot between a percentage of each page for churned and not churned users below shows that the page of most events are 'NextPage' and the plot does not show much difference for each page type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get page count of events with and without churns\n",
    "df_churn_page = df_valid.groupby('willBeChurned', 'page') \\\n",
    "    .count() \\\n",
    "    .orderBy(desc('willBeChurned'), desc('count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And inspect the raw data\n",
    "df_churn_page.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results by converting data to pandas first\n",
    "pd_churn_page = df_churn_page.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, find a total counts of each type\n",
    "total_churn_page = pd_churn_page.groupby('willBeChurned')['count'].sum()\n",
    "total_churn_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, create a new column called 'percentage' to store a percentage number of each page count\n",
    "def get_page_count_percentage(row):\n",
    "    return (row['count'] / total_churn_page[row['willBeChurned']])*100\n",
    "\n",
    "pd_churn_page['percentage'] = pd_churn_page.apply(lambda row: get_page_count_percentage(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a pivot table to better visualize the data\n",
    "pd_churn_page.pivot(index='willBeChurned', columns='page', values='percentage')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, use the newly created pandas data frame to create a comparison plot\n",
    "# to show a difference between a percentage of each page type by churn\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "sns.barplot(x='percentage', y='page', hue='willBeChurned', data=pd_churn_page);\n",
    "plt.title(\"Comparison of a percentage of event counts of each page type for events with/without churn\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect a number of songs played in each hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below shows a similar trend between a number of songs played in each hour of events of users who are/are not churned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat an hour column by calculating Static hour using the 'ts' column\n",
    "get_hour = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).hour)\n",
    "df_valid = df_valid.withColumn('hour', get_hour('ts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_NumberOfSongsInHour(willBeChurned):\n",
    "    '''\n",
    "    Plot a number of songs in hour for events with the specified willBeChurned value\n",
    "    '''\n",
    "    \n",
    "    # Get a number of songs in hour for a particular data set\n",
    "    songs_in_hour = df_valid.filter(df_valid.willBeChurned == willBeChurned) \\\n",
    "        .groupby('hour') \\\n",
    "        .count() \\\n",
    "        .orderBy(df_valid.hour.cast(\"float\"))\n",
    "\n",
    "    \n",
    "    # Convert to pandas and create a scatter plot\n",
    "\n",
    "    pd_songs_in_hour = songs_in_hour.toPandas()\n",
    "    pd_songs_in_hour.hour = pd.to_numeric(pd_songs_in_hour.hour)\n",
    "\n",
    "    plt.scatter(pd_songs_in_hour[\"hour\"], pd_songs_in_hour[\"count\"]);\n",
    "    plt.xlim(-1, 24);\n",
    "    plt.ylim(0, 1.2 * pd_songs_in_hour[\"count\"].max())\n",
    "    plt.xlabel(\"Hour\")\n",
    "    plt.ylabel(\"Songs played\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot to inspect The number of songs in hours of events of not churned users\n",
    "plot_NumberOfSongsInHour(0)\n",
    "plt.title('The number of songs in hours of events of NOT churned users');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot to inspect The number of songs in hours of events of not churned users\n",
    "plot_NumberOfSongsInHour(1)\n",
    "plt.title('The number of songs in hours of events of CHURNED users');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect a ratio of each gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data below shows that it is more likely for Male to be churned than a female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender = df_valid.select(\"userId\", \"gender\", 'willBeChurned') \\\n",
    "    .dropDuplicates() \\\n",
    "    .groupby('willBeChurned', 'gender') \\\n",
    "    .count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender.orderBy('willBeChurned', 'gender').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_gender = df_gender.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_gender_pivot = pd_gender.pivot(index='willBeChurned', columns='gender', values='count')\n",
    "total_counts = pd_gender_pivot.sum(axis=1)\n",
    "pd_gender_pivot = pd_gender_pivot.div(total_counts, axis=0)\n",
    "pd_gender_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_gender_pivot.plot.barh();\n",
    "plt.title('Comparison of ratio of gender counts of users who are/are not churned');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect session time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistics of session time below shows that \n",
    "* The average of session time of users who are churned is around 283 minutes/session \n",
    "* The the average of session time of users who are NOT  churned is around 302 minutes/session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data based on its willBeChurned status first \n",
    "df_churn = df_valid.filter('willBeChurned == 1')\n",
    "df_not_churn = df_valid.filter('willBeChurned == 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_time(df, showSummary=False):\n",
    "    '''\n",
    "    Get a number of registration days of the given data\n",
    "    \n",
    "    Input Argument:\n",
    "        df:   Sparkify dataframe\n",
    "        \n",
    "    Optional Input Argument:\n",
    "        showSummary: Show summary statistics of the output data fram. Default is False.\n",
    "        \n",
    "    Output Argument:\n",
    "        df_sessionTime: Spark dataframe contains sessionTime information of each userId and sessionId\n",
    "    '''\n",
    "    \n",
    "    # Find a session time by grouping events by userId and sessionId\n",
    "    # and then find a difference between the min and max timestamp\n",
    "    df_sessionTime = df.groupby(\"userId\", \"sessionId\") \\\n",
    "        .agg(((max(df.ts)-min(df.ts))/(1000*60)) \\\n",
    "        .alias(\"sessionTime\"))\n",
    "    \n",
    "    if showSummary:\n",
    "        # Print the statistics\n",
    "        df_sessionTime.select('sessionTime').describe().show();\n",
    "\n",
    "        # Create a box plot\n",
    "        df_sessionTime.select('sessionTime').toPandas().boxplot();\n",
    "    \n",
    "    return df_sessionTime;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sessionTime_churn = get_session_time(df_churn, showSummary=True)\n",
    "plt.title('Distribution of session time of users who are churned');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sessionTime_not_churn = get_session_time(df_not_churn, showSummary=True)\n",
    "plt.title('Distribution of session time of users who are NOT churned');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect a number of songs per session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistics of session time below shows that \n",
    "* The average of songs played in each session of users who are churned is around 70 songs\n",
    "* The average of songs played in each session of users who are NOT churned is around 75 songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_songs_per_session(df, showSummary=False):\n",
    "    '''\n",
    "    Get a number of songs played in each userId and sessionId\n",
    "    \n",
    "    Input Argument:\n",
    "        df:   Sparkify dataframe\n",
    "        \n",
    "    Optional Input Argument:\n",
    "        showSummary: Show summary statistics of the output data fram. Default is False.\n",
    "        \n",
    "    Output Argument:\n",
    "        df_songs_per_session: Spark dataframe contains information of each userId and sessionId\n",
    "    '''\n",
    "    \n",
    "    # Filter only events with the \"NextSong\" page\n",
    "    # Then, group them by userId and sessionId and find the counts\n",
    "    df_songs_per_session = df.filter(df.page==\"NextSong\") \\\n",
    "        .groupby(\"userId\", \"sessionId\") \\\n",
    "        .count()\n",
    "    \n",
    "    if showSummary:\n",
    "        # Print the statistics\n",
    "        df_songs_per_session.select('count').describe().show();\n",
    "\n",
    "        # Create a box plot\n",
    "        df_songs_per_session.select('count').toPandas().boxplot();\n",
    "    \n",
    "    return df_songs_per_session;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs_per_session_churn = get_songs_per_session(df_churn, showSummary=True);\n",
    "plt.title('Distribution of a number of songs per session of users who are churned');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs_per_session_not_churn = get_songs_per_session(df_not_churn, showSummary=True)\n",
    "plt.title('Distribution of a number of songs per session of users who are churned');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect a number of registration days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data below shows that a number of registration days of users who are churned are around 57 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_reg_days(df, showSummary=False):\n",
    "    '''\n",
    "    Get a number of registration days of the given data\n",
    "    \n",
    "    Input Argument:\n",
    "        df:   Sparkify dataframe\n",
    "        \n",
    "    Optional Input Argument:\n",
    "        showSummary: Show summary statistics of the output data fram. Default is False.\n",
    "        \n",
    "    Output Argument:\n",
    "        user_reg_days: Spark dataframe contains information of a number of registration days\n",
    "    '''\n",
    "    \n",
    "    # Get a max timestamp of each ID first\n",
    "    user_max_ts = df.groupby(\"userId\").max(\"ts\").sort(\"userId\")\n",
    "\n",
    "    # Get the registration timestamp. We only need to get it from the first data \n",
    "    # since they will all be the same\n",
    "    user_reg_ts = df.select(\"userId\", \"registration\").dropDuplicates().sort(\"userId\")\n",
    "\n",
    "    # Join the max timestamp and registration timestamp table\n",
    "    join_reg_max_ts = user_reg_ts.join(user_max_ts, (user_reg_ts.userId == user_max_ts.userId))\n",
    "\n",
    "    # Then, select\n",
    "    # 1) The userId column\n",
    "    # 2) Computed a number of registration days from the max and registration columns\n",
    "    user_reg_days = join_reg_max_ts.select(user_reg_ts[\"userId\"], \n",
    "                                           ((user_max_ts[\"max(ts)\"]-user_reg_ts[\"registration\"])/(1000*60*60*24))\n",
    "                                            .alias(\"NumRegDays\"))\n",
    "    \n",
    "    if showSummary:\n",
    "        # Print the statistics\n",
    "        user_reg_days.select('NumRegDays').describe().show();\n",
    "\n",
    "        # Create a boxplot of the statistics\n",
    "        user_reg_days.select('NumRegDays').toPandas().boxplot();\n",
    "\n",
    "    return user_reg_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a number of registration days and statistics of users who are churned\n",
    "user_reg_days = get_user_reg_days(df_churn, showSummary=True)\n",
    "plt.title('The number of registration days of users who are churned ');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a number of registration days and statistics of users who are NOT churned\n",
    "user_reg_days = get_user_reg_days(df_not_churn, showSummary=True)\n",
    "plt.title('The number of registration days of users who are churned ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the last level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data below shows that a ratio of free/paid users who are/are not churned are similar (around 55/45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_level(df, showSummary=False):\n",
    "    '''\n",
    "    Get data of the last user level (paid/free) of a given data\n",
    "    \n",
    "    Input Argument:\n",
    "        df:   Sparkify dataframe\n",
    "    \n",
    "    Optional Input Argument:\n",
    "        showSummary: Show summary statistics of the output data fram. Default is False.\n",
    "        \n",
    "    Output Argument:\n",
    "        user_reg_days: Spark dataframe contains data of the last user level (paid/free) of a given data\n",
    "    '''\n",
    "    \n",
    "    # Get a max timestamp of each ID first\n",
    "    user_max_ts = df.groupby(\"userId\").max(\"ts\").sort(\"userId\")\n",
    "\n",
    "    # Get the registration timestamp. We only need to get it from the first data \n",
    "    # since they will all be the same\n",
    "    user_level = df.select(\"userId\", \"level\").dropDuplicates()\n",
    "    \n",
    "    \n",
    "    if showSummary:\n",
    "        # we will show a summary of counts grouped by 'level' here\n",
    "        total_user_level = user_level.count()\n",
    "\n",
    "        group_level = user_level.groupby('level').count()\n",
    "        get_ratio = udf(lambda x: x*1.0/total_user_level, FloatType())\n",
    "        df_level = group_level.withColumn('ratio', get_ratio(\"count\"))\n",
    "        df_level.show()\n",
    "    \n",
    "    return user_level;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a number of last level of users who are churned\n",
    "get_user_level(df_churn, showSummary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a number of last level of users who are NOT churned\n",
    "get_user_level(df_not_churn, showSummary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the Exploratory Data Analysis (EDA) above, we will use the following information to create features that will be used to train our model:\n",
    "* Gender of the user\n",
    "* Average session time of each user\n",
    "* Average number of songs played in each session of each user\n",
    "* How long the user has been registered\n",
    "* The last level (paid/free)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gender of the user**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate first because the gender information will not be changed\n",
    "df_gender = df_valid.dropDuplicates(['userId']).sort('userId').select(['userId','gender'])\n",
    "\n",
    "# Then, use StringIndexerModel to create an index for gender\n",
    "\n",
    "fromlabelsModel = StringIndexerModel.from_labels([\"F\", \"M\"], \\\n",
    "    inputCol=\"gender\", outputCol=\"gender_index\", handleInvalid=\"error\")\n",
    "df_gender = fromlabelsModel.transform(df_gender)\n",
    "\n",
    "# Only extracted interested columns\n",
    "df_gender = df_gender.select('userId', df_gender.gender_index.cast(\"int\"))\n",
    "df_gender.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average session time of each user**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_column_with_StandardScaler(df, column_name):\n",
    "    '''\n",
    "    Scale the specified column name with the StandardScaler\n",
    "    \n",
    "    Input arguments:\n",
    "        df:           Spark dataframe\n",
    "        column_name:  Column name with numerical value to be scaled with the StandardScaler\n",
    "    \n",
    "    Output argument:\n",
    "        df_scaled:    New Spark dataframe with the \"<column_name>_vec\" and \"<column_name>_scaled\" columns\n",
    "    '''\n",
    "    \n",
    "    # Create a vector of sessionTime first before scaling it\n",
    "    column_vec = column_name + '_vec'\n",
    "    assembler = VectorAssembler(inputCols=[column_name], outputCol=column_vec)\n",
    "    df_scaled = assembler.transform(df)\n",
    "\n",
    "    # Use the StandardScale to scale the session time\n",
    "    column_scaled = column_name + '_scaled'\n",
    "    scaler = StandardScaler(inputCol=column_vec, \n",
    "                            outputCol=column_scaled, \n",
    "                            withStd=True)\n",
    "\n",
    "    # Then, select only the interested columns\n",
    "    df_scaled = scaler.fit(df_scaled).transform(df_scaled)\n",
    "    \n",
    "    return df_scaled;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_column_by_user(df, column_name):\n",
    "    '''\n",
    "    Get average values of a particular column grouped by userId\n",
    "    \n",
    "    Input argument:\n",
    "    df :  Sparkify dataframe\n",
    "    column_name: The interested column name that we want to get average values for\n",
    "    \n",
    "    Output argument:\n",
    "    df_avg: Spark dataframe with average values information stored in a column \"avg_<column_name>\"\"\n",
    "    '''\n",
    "    df_avg = df.groupby('userId') \\\n",
    "                .agg(avg(column_name)) \\\n",
    "                .select('userId', col('avg(' + column_name + ')').alias('avg_' + column_name))\n",
    "    \n",
    "    return df_avg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the average session time first\n",
    "df_sessionTime = get_session_time(df_valid)\n",
    "df_sessionTime_avg = get_avg_column_by_user(df_sessionTime, 'sessionTime')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the results\n",
    "df_sessionTime_avg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, scaled it\n",
    "df_sessionTime_avg_scaled = scale_column_with_StandardScaler(df_sessionTime_avg, 'avg_sessionTime')\n",
    "df_sessionTime_avg_scaled = df_sessionTime_avg_scaled.select('userId', 'avg_sessionTime_scaled')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the result\n",
    "df_sessionTime_avg_scaled.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average number of songs played in each session of each user**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data of a number of songs per session\n",
    "df_song = get_songs_per_session(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an average number of songs per session of each userId\n",
    "df_song_avg = get_avg_column_by_user(df_song, 'count')\n",
    "# Rename the column to be more specific\n",
    "df_song_avg = df_song.select('userId', col('count').alias('num_songs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, scale data in the 'num_songs' column with the StandardScaler\n",
    "df_song_avg_scaled = scale_column_with_StandardScaler(df_song_avg, 'num_songs')\n",
    "\n",
    "# And only select the userId and the scaled column\n",
    "df_song_avg_scaled = df_song_avg_scaled.select('userId', 'num_songs_scaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the data\n",
    "df_song_avg_scaled.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How long the user has been registered**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data of a number of days since each user registered\n",
    "df_reg_days = get_user_reg_days(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, scale it with the StandardScaler\n",
    "df_reg_days_scaled = scale_column_with_StandardScaler(df_reg_days, 'NumRegDays')\n",
    "df_reg_days_scaled = df_reg_days_scaled.select('userId', 'NumRegDays_scaled')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the data\n",
    "df_reg_days_scaled.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The last level (paid/free)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data of the last level of each user\n",
    "df_level = get_user_level(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, transform it\n",
    "\n",
    "fromlabelsModel = StringIndexerModel.from_labels([\"free\", \"paid\"], \\\n",
    "    inputCol=\"level\", outputCol=\"level_index\", handleInvalid=\"error\")\n",
    "df_level = fromlabelsModel.transform(df_level)\n",
    "\n",
    "# Only extracted interested columns\n",
    "df_level = df_level.select('userId', df_level.level_index.cast(\"int\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect data\n",
    "df_level.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After transforming/scaling the interested columns, join them with dataframe with the churn dataframe using the 'userId' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the label (churn) data first\n",
    "df_combined = df_valid.dropDuplicates(['userId']) \\\n",
    "                .sort('userId') \\\n",
    "                .select(['userId', col('willBeChurned').alias('label').cast(\"int\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each data frame that we created earlier and joining them with the label dataframe\n",
    "feature_list = [    \n",
    "    df_gender, \n",
    "    df_sessionTime_avg_scaled, \n",
    "    df_song_avg_scaled, \n",
    "    df_reg_days_scaled, \n",
    "    df_level\n",
    "]\n",
    "\n",
    "for feature in feature_list:\n",
    "    df_combined = df_combined.join(feature,'userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the schema of the combined data frame first\n",
    "df_combined.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a dataframe that will be used in the modeling step by \n",
    "1. Combining values in the feature columns into a single DenseVector and named it 'features'\n",
    "2. Create a data frame 'df_final' by selecting only the 'features' and 'label' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=df_combined.columns[2:-1], outputCol=\"features\")\n",
    "df_combined = assembler.transform(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_combined.select('features', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out data for a future use\n",
    "out_path = \"final_data.csv\"\n",
    "df_final.write.save(out_path, format=\"csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final = spark.read.csv(out_path, header=True)\n",
    "df_final.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's split the data set into 80% of training data and set aside 20%. Set random seed to 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_final.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first start with the LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr =  LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we already transform data in the previous step, \n",
    "# we will not need to do it in the pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, create a ParamGridBuilder\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.elasticNetParam,[0.0, 0.5, 1.0]) \\\n",
    "    .addGrid(lr.regParam,[0.0, 0.01, 0.1]) \\\n",
    "    .build()\n",
    "\n",
    "# And use pipeline and paramGrid to construct a CrossValidator object\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit with the train data\n",
    "print('[{}] Start'.format(datetime.datetime.now()))\n",
    "\n",
    "cvModel_lr = crossval.fit(train)\n",
    "\n",
    "print('[{}] Done'.format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the average metrics\n",
    "cvModel_lr.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, get a predicted value of the test data\n",
    "results_lr = cvModel_lr.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, check the accuracy of the results\n",
    "print(results_lr.filter(results_lr.label == results_lr.prediction).count())\n",
    "print(results_lr.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient-boosted tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
